<included>
    <property scope="context" name="CHARSET" value="utf-8"/>
    <!--<property scope="context" name="appName" value="{{tomcat.projectName}}"/>-->
    <!--<property scope="context" name="kafkaServers" value="{{tomcat.kafkaServers}}"/>-->
    <!--<property scope="context" name="LOG_HOME" value="{{tomcat.logHome}}/{{tomcat.projectName}}" />-->
    <property scope="context" name="intflogType" value="INTF_LOG"/>
    <property scope="context" name="applogType" value="APP_LOG"/>
    <property scope="context" name="srvlogType" value="SRV_LOG"/>
    <property scope="context" name="APP_LOG_HOME" value="${LOG_HOME}/app/" />
    <property scope="context" name="INF_LOG_HOME" value="${LOG_HOME}/inf/" />
    <property scope="context" name="SRV_LOG_HOME" value="${LOG_HOME}/srv/" />

    <!-- 接口日志_fileAppender -->
    <appender name="INTF_FileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${INF_LOG_HOME}/${HOSTNAME}.inf.ing</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${INF_LOG_HOME}/${appName}_intf_log.${HOSTNAME}.%d{yyyy-MM-dd_HH}.json.%i</fileNamePattern>
            <MaxHistory>1440</MaxHistory><!--日志文件保存个数 -->
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <!-- 按时间回滚的同时，按文件大小来回滚 -->
                <maxFileSize>30MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <fieldNames class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            <shortenedLoggerNameLength>36</shortenedLoggerNameLength>
            <includeContext>false</includeContext>
            <includeMdc>false</includeMdc>
            <includeCallerInfo>false</includeCallerInfo>
            <customFields>{"host": "${HOSTNAME}", "appName": "${appName}"}</customFields>
        </encoder>
    </appender>
    <!-- 接口日志_kafkaAppender -->
    <appender name="intfLogAppender" class="com.wanmi.ms.log.appender.LogstashKafkaAppender">
        <queueSize>1024</queueSize>                      <!-- default buffer size, default is 1024 -->
        <discardingThreshold>204</discardingThreshold>   <!-- if queue remaining capacity less then this value, debug and info will be discard. default is queueSize/5 -->
        <topic>LOGSTASH_${intflogType}</topic>                         <!-- topic name, required -->
        <daemonThread>true</daemonThread>                <!-- set executor thread mode; default is true -->
        <maximumFrequency>1000</maximumFrequency>       <!-- maximum count of events per second, default is 10000 -->
        <ignoreOverload>false</ignoreOverload>           <!-- if true ignore overload and continue write to kafka; default is false-->
        <producerConfig>bootstrap.servers=${kafkaServers}}</producerConfig>           <!-- kafka producer config, required -->
        <producerConfig>acks=1</producerConfig>
        <producerConfig>metadata.fetch.timeout.ms=1000</producerConfig>
        <producerConfig>metadata.max.age.ms=1000</producerConfig>
        <layout class="net.logstash.logback.layout.LogstashLayout">
            <fieldNames class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            <shortenedLoggerNameLength>36</shortenedLoggerNameLength>
            <includeContext>false</includeContext>
            <includeMdc>false</includeMdc>
            <includeCallerInfo>false</includeCallerInfo>
            <customFields>{"host": "${HOSTNAME}", "appName": "${appName}"}</customFields>
        </layout>
        <appender-ref ref="INTF_FileAppender" />       <!-- output to this appender when output to kafka failed; if you are not configured, then ignore this log, optional; -->
    </appender>

    <!-- 应用日志_fileAppender -->
    <appender name="APP_FileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${APP_LOG_HOME}/${HOSTNAME}.app.ing</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${APP_LOG_HOME}/${appName}_app_log.${HOSTNAME}.%d{yyyy-MM-dd_HH}.json.%i</fileNamePattern>
            <MaxHistory>1440</MaxHistory><!--日志文件保存个数 -->
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <!-- 按时间回滚的同时，按文件大小来回滚 -->
                <maxFileSize>30MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <fieldNames class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            <shortenedLoggerNameLength>36</shortenedLoggerNameLength>
            <includeContext>false</includeContext>
            <includeMdc>true</includeMdc>
            <includeCallerInfo>true</includeCallerInfo>
            <customFields>{"host": "${HOSTNAME}", "appName": "${appName}"}</customFields>
        </encoder>
    </appender>
    <!-- 应用日志_kafkaAppender -->
    <appender name="appLogAppender" class="com.wanmi.ms.log.appender.LogstashKafkaAppender">
        <queueSize>1024</queueSize>                      <!-- default buffer size, default is 1024 -->
        <discardingThreshold>204</discardingThreshold>   <!-- if queue remaining capacity less then this value, debug and info will be discard. default is queueSize/5 -->
        <topic>LOGSTASH_${applogType}</topic>                         <!-- topic name, required -->
        <daemonThread>true</daemonThread>                <!-- set executor thread mode; default is true -->
        <maximumFrequency>1000</maximumFrequency>       <!-- maximum count of events per second, default is 10000 -->
        <ignoreOverload>false</ignoreOverload>           <!-- if true ignore overload and continue write to kafka; default is false-->
        <producerConfig>bootstrap.servers=${kafkaServers}}</producerConfig>           <!-- kafka producer config, required -->
        <producerConfig>acks=1</producerConfig>
        <producerConfig>metadata.fetch.timeout.ms=1000</producerConfig>
        <producerConfig>metadata.max.age.ms=1000</producerConfig>
        <layout class="net.logstash.logback.layout.LogstashLayout">
            <fieldNames class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            <shortenedLoggerNameLength>36</shortenedLoggerNameLength>
            <includeContext>false</includeContext>
            <includeMdc>true</includeMdc>
            <includeCallerInfo>true</includeCallerInfo>
            <customFields>{"host": "${HOSTNAME}", "appName": "${appName}"}</customFields>
        </layout>
        <appender-ref ref="APP_FileAppender" />       <!-- output to this appender when output to kafka failed; if you are not configured, then ignore this log, optional; -->
    </appender>

    <!-- 业务日志_fileAppender -->
    <appender name="SRV_FileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${SRV_LOG_HOME}/${HOSTNAME}.inf.ing</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${SRV_LOG_HOME}/${appName}_srv_log.${HOSTNAME}.%d{yyyy-MM-dd_HH}.json.%i</fileNamePattern>
            <MaxHistory>1440</MaxHistory><!--日志文件保存个数 -->
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <!-- 按时间回滚的同时，按文件大小来回滚 -->
                <maxFileSize>30MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <fieldNames class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            <shortenedLoggerNameLength>36</shortenedLoggerNameLength>
            <includeContext>false</includeContext>
            <includeMdc>false</includeMdc>
            <includeCallerInfo>false</includeCallerInfo>
            <customFields>{"host": "${HOSTNAME}", "appName": "${appName}"}</customFields>
        </encoder>
    </appender>
    <!-- 业务日志_kafkaAppender -->
    <appender name="srvLogAppender" class="com.wanmi.ms.log.appender.LogstashKafkaAppender">
        <queueSize>1024</queueSize>                      <!-- default buffer size, default is 1024 -->
        <discardingThreshold>204</discardingThreshold>   <!-- if queue remaining capacity less then this value, debug and info will be discard. default is queueSize/5 -->
        <topic>LOGSTASH_${srvlogType}</topic>                         <!-- topic name, required -->
        <daemonThread>true</daemonThread>                <!-- set executor thread mode; default is true -->
        <maximumFrequency>1000</maximumFrequency>       <!-- maximum count of events per second, default is 10000 -->
        <ignoreOverload>false</ignoreOverload>           <!-- if true ignore overload and continue write to kafka; default is false-->
        <producerConfig>bootstrap.servers=${kafkaServers}}</producerConfig>           <!-- kafka producer config, required -->
        <producerConfig>acks=1</producerConfig>
        <producerConfig>metadata.fetch.timeout.ms=1000</producerConfig>
        <producerConfig>metadata.max.age.ms=1000</producerConfig>
        <layout class="net.logstash.logback.layout.LogstashLayout">
            <fieldNames class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            <shortenedLoggerNameLength>36</shortenedLoggerNameLength>
            <includeContext>false</includeContext>
            <includeMdc>false</includeMdc>
            <includeCallerInfo>false</includeCallerInfo>
            <customFields>{"host": "${HOSTNAME}", "appName": "${appName}"}</customFields>
        </layout>
        <appender-ref ref="SRV_FileAppender" />       <!-- output to this appender when output to kafka failed; if you are not configured, then ignore this log, optional; -->
    </appender>

</included>